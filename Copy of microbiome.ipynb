{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"microbiome.ipynb","provenance":[{"file_id":"https://github.com/raqueldias/ALS3200C/blob/main/microbiome.ipynb","timestamp":1699853601898}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Mc5xkI2hXrwh"},"source":["# microbial community disease risk prediction\n","\n","In this case study, we'll develop a neural network to predict disease risk from microbial community sequence data.\n","\n","We have 16S rDNA sequence data from 16,344 samples, roughly half of which are from individuals who have been diagnosed with type 1 diabetes (aka, \"cases\"), and half of which are from individuals who do not have type 1 diabetes (\"controls\").\n","\n","The data are available in a github repository as a comma-separated values (.csv) file. So, we can use the pandas library to downoad the sequence data and associated disease-state labels to a pandas dataframe:"]},{"cell_type":"code","metadata":{"id":"alDR3GTbXqOw","executionInfo":{"status":"ok","timestamp":1699852807026,"user_tz":300,"elapsed":1557,"user":{"displayName":"","userId":""}},"outputId":"c6c21b8e-ee92-4cc7-d1e5-5ba04004275a","colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["import pandas\n","dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n","dataframe.head()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   DTA0  DTA1  DTA2  DTA3  DTA4  DTA5  DTA6  DTA7  DTA8  DTA9  ...  DTA247  \\\n","0  1.92  1.80  1.44  1.79  1.68  1.42  1.52  1.58  1.43  1.45  ...   -2.02   \n","1  1.97  1.98  2.16  2.12  1.78  1.71  1.69  1.60  1.74  1.64  ...   -2.05   \n","2  2.25  2.11  2.05  1.92  2.08  1.93  1.87  1.57  1.81  1.61  ...   -2.02   \n","3  2.25  2.07  1.92  1.84  1.83  1.80  1.88  1.48  1.70  1.46  ...   -1.94   \n","4  2.28  2.27  2.26  2.20  2.01  2.00  1.99  1.92  1.68  1.79  ...   -1.69   \n","\n","   DTA248  DTA249  DTA250  DTA251  DTA252  DTA253  DTA254  DTA255  LBL0  \n","0   -2.32   -2.19   -2.25   -2.25   -2.29   -2.19   -2.63   -2.86   1.0  \n","1   -1.97   -1.92   -2.12   -1.94   -2.18   -2.45   -2.63   -2.87   0.0  \n","2   -1.87   -1.95   -2.09   -1.96   -1.99   -2.01   -2.57   -2.71   1.0  \n","3   -2.11   -2.22   -1.98   -2.22   -2.00   -2.10   -2.59   -2.84   0.0  \n","4   -1.66   -1.82   -1.88   -1.92   -1.89   -2.07   -2.50   -2.72   0.0  \n","\n","[5 rows x 257 columns]"],"text/html":["\n","  <div id=\"df-2e50aa4f-5a11-4b82-b63f-9ba19a8c962e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DTA0</th>\n","      <th>DTA1</th>\n","      <th>DTA2</th>\n","      <th>DTA3</th>\n","      <th>DTA4</th>\n","      <th>DTA5</th>\n","      <th>DTA6</th>\n","      <th>DTA7</th>\n","      <th>DTA8</th>\n","      <th>DTA9</th>\n","      <th>...</th>\n","      <th>DTA247</th>\n","      <th>DTA248</th>\n","      <th>DTA249</th>\n","      <th>DTA250</th>\n","      <th>DTA251</th>\n","      <th>DTA252</th>\n","      <th>DTA253</th>\n","      <th>DTA254</th>\n","      <th>DTA255</th>\n","      <th>LBL0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.92</td>\n","      <td>1.80</td>\n","      <td>1.44</td>\n","      <td>1.79</td>\n","      <td>1.68</td>\n","      <td>1.42</td>\n","      <td>1.52</td>\n","      <td>1.58</td>\n","      <td>1.43</td>\n","      <td>1.45</td>\n","      <td>...</td>\n","      <td>-2.02</td>\n","      <td>-2.32</td>\n","      <td>-2.19</td>\n","      <td>-2.25</td>\n","      <td>-2.25</td>\n","      <td>-2.29</td>\n","      <td>-2.19</td>\n","      <td>-2.63</td>\n","      <td>-2.86</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.97</td>\n","      <td>1.98</td>\n","      <td>2.16</td>\n","      <td>2.12</td>\n","      <td>1.78</td>\n","      <td>1.71</td>\n","      <td>1.69</td>\n","      <td>1.60</td>\n","      <td>1.74</td>\n","      <td>1.64</td>\n","      <td>...</td>\n","      <td>-2.05</td>\n","      <td>-1.97</td>\n","      <td>-1.92</td>\n","      <td>-2.12</td>\n","      <td>-1.94</td>\n","      <td>-2.18</td>\n","      <td>-2.45</td>\n","      <td>-2.63</td>\n","      <td>-2.87</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.25</td>\n","      <td>2.11</td>\n","      <td>2.05</td>\n","      <td>1.92</td>\n","      <td>2.08</td>\n","      <td>1.93</td>\n","      <td>1.87</td>\n","      <td>1.57</td>\n","      <td>1.81</td>\n","      <td>1.61</td>\n","      <td>...</td>\n","      <td>-2.02</td>\n","      <td>-1.87</td>\n","      <td>-1.95</td>\n","      <td>-2.09</td>\n","      <td>-1.96</td>\n","      <td>-1.99</td>\n","      <td>-2.01</td>\n","      <td>-2.57</td>\n","      <td>-2.71</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.25</td>\n","      <td>2.07</td>\n","      <td>1.92</td>\n","      <td>1.84</td>\n","      <td>1.83</td>\n","      <td>1.80</td>\n","      <td>1.88</td>\n","      <td>1.48</td>\n","      <td>1.70</td>\n","      <td>1.46</td>\n","      <td>...</td>\n","      <td>-1.94</td>\n","      <td>-2.11</td>\n","      <td>-2.22</td>\n","      <td>-1.98</td>\n","      <td>-2.22</td>\n","      <td>-2.00</td>\n","      <td>-2.10</td>\n","      <td>-2.59</td>\n","      <td>-2.84</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.28</td>\n","      <td>2.27</td>\n","      <td>2.26</td>\n","      <td>2.20</td>\n","      <td>2.01</td>\n","      <td>2.00</td>\n","      <td>1.99</td>\n","      <td>1.92</td>\n","      <td>1.68</td>\n","      <td>1.79</td>\n","      <td>...</td>\n","      <td>-1.69</td>\n","      <td>-1.66</td>\n","      <td>-1.82</td>\n","      <td>-1.88</td>\n","      <td>-1.92</td>\n","      <td>-1.89</td>\n","      <td>-2.07</td>\n","      <td>-2.50</td>\n","      <td>-2.72</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 257 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e50aa4f-5a11-4b82-b63f-9ba19a8c962e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2e50aa4f-5a11-4b82-b63f-9ba19a8c962e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2e50aa4f-5a11-4b82-b63f-9ba19a8c962e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cf29625b-e524-4a65-8d54-a24da93438f9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cf29625b-e524-4a65-8d54-a24da93438f9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cf29625b-e524-4a65-8d54-a24da93438f9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"mlKLWh5rXq9g"},"source":["There are 256 \"DTA\" columns, lebelled DTA0, ..., DTA255. Each of these DTA columns represents a particular bacterial \"species\" found in the samples. The 'relative abundance' of each species (column) in each sample (row) is reported. Relative abundance values have been center log-ratio transformed, which is a common method used to 'normalize' microbial relative abundance data.\n","\n","In a typical analysis of 16S rDNA sequence data, the 'relative abundance' of each sequence in the sample is given as the *number* of sequence reads matching that sequence in the sample. One *could* divide each sequence count by the total number of counts in that sample, which would produce a typical 'relative abundance' value between 0.0 (not found in the sample) and 1.0 (the *only* sequence found in the sample).\n","\n","However, it's more common to perform some sort of log-ratio transform of the sequence count data. Log-ratio transforms have a couple of advantages over the 'frequency transform' above. First, putting numbers on a log scale often makes them more 'normally distributed', which typically provides a better fit to the assumptions of most statistical models. Second, the log scale can be 'centered' at zero, with positive and negative values indicating deviations from the 'average' value of zero; this 'centering' often leads to better results from machine-learning and neural-network models.\n","\n","The center log-ratio transform is simple to calculate and is commonly used for microbial community sequence projects. One simply divides each sequence's count by the *geometric* mean of the total counts over all sequences in the sample, and then takes the logarithm of this 'ratio'.\n","\n","These data have already been center log-ratio transformed, and you can see that the values typically range between about +2.5 and -2.5.\n","\n","The final column in the data file, labelled \"LBL0\" is the 'disease state' indicator (the label we'd like to predict), with 0 indicating a 'control' individual with no type 1 diabetes diagnosis, and 1 indicating a 'case' individual who has been diagnosed with type 1 diabetes.\n","\n","Our goal is to predict the LBL0 classification, given the microbial sequence information in columns DTA0, ..., DTA255.\n","\n","First, let's split our data into training and validation subsets, and extract the explanatory variables and labels.\n","\n","Much of the following code cell should look familiar. Given the pandas dataframe, we first sample 80% of the data for training, and leave the remaining 20% for validation.\n","\n","Next, we extract the columns starting with \"DTA\" as the explanatory variables. In this case, we need to 'expand' the data dimension, so we can model these data using a tensorflow sequence model (like a Conv1D or LSTM model).\n","\n","Finally, we extract the LBL0 entries as our binary class labels."]},{"cell_type":"code","metadata":{"id":"XVTUJdVQYDkv","executionInfo":{"status":"ok","timestamp":1699852841345,"user_tz":300,"elapsed":155,"user":{"displayName":"","userId":""}},"outputId":"3bbb2019-92e2-4fb5-e4ee-b619661fe76c","colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","\n","# create train-validate split\n","train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n","valid_dataframe = dataframe.drop(train_dataframe.index)\n","print(train_dataframe.shape, valid_dataframe.shape, dataframe.shape)\n","\n","# extract explanatory variables\n","dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n","train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n","valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n","print(train_x.shape, valid_x.shape)\n","\n","# extract labels\n","train_y = train_dataframe['LBL0'].to_numpy()\n","valid_y = valid_dataframe['LBL0'].to_numpy()\n","print(train_y.shape, valid_y.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["(13075, 257) (3269, 257) (16344, 257)\n","(13075, 256, 1) (3269, 256, 1)\n","(13075,) (3269,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"a8CPzypUY4TL"},"source":["We can see that there are 16,344 total samples in our dataframe. We've extracted 13,057 for training and 3,269 for validation.\n","\n","After expanding the data dimension, we have explanatory variables of shape (256,1), a one-dimensional sequence of 256 bacterial 'species'."]},{"cell_type":"markdown","metadata":{"id":"zYabR-E7fsni"},"source":["## simple linear model\n","\n","Now that we have some training and validation data, we just need to build a classifier to predict disease risk from the data.\n","\n","First, we'll start with a simple linear model implemented using a single Dense neuron with sigmoid output, as this is a binary classification problem.\n","\n","In previous cases, we were able to send the data directly to the Dense layer. But in this case, because we have 'expanded' the last dimension of the data - in order to fit tensorflow's sequence models - we need to 'collapse' that dimension back down, so it can be properly analyzed by the Dense layer.\n","\n","It's pretty easy to do this; we just need to use a Flatten layer to 'flatten' the 'expanded' data back down to a simple vector. And, because the Flatten layer is just like any other tensorflow Layer object, we can use it as the *first* layer in our network, provided we set the input_shape option.\n","\n","The following code cell implements a simple linear classifier for our disease-risk prediction problem."]},{"cell_type":"code","metadata":{"id":"ldKzbLpyXIAW","executionInfo":{"status":"ok","timestamp":1699853094487,"user_tz":300,"elapsed":50329,"user":{"displayName":"","userId":""}},"outputId":"2f346319-2250-4c1b-f469-6682973a9662","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas\n","import numpy as np\n","import tensorflow as tf\n","\n","# download data\n","dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n","\n","# create train-validate split\n","train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n","valid_dataframe = dataframe.drop(train_dataframe.index)\n","\n","# extract explanatory variables\n","dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n","train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n","valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n","\n","# extract labels\n","train_y = train_dataframe['LBL0'].to_numpy()\n","valid_y = valid_dataframe['LBL0'].to_numpy()\n","\n","# package data into tensorflow dataset\n","train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n","valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n","\n","# build model\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Flatten(input_shape=(256,1)))\n","model.add(tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid))\n","model.summary()\n","\n","# compile model\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","# fit model\n","model.fit(train_data, epochs=20, validation_data=valid_data)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 257 (1.00 KB)\n","Trainable params: 257 (1.00 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","409/409 [==============================] - 7s 4ms/step - loss: 0.6829 - accuracy: 0.5570 - val_loss: 0.6821 - val_accuracy: 0.5240\n","Epoch 2/20\n","409/409 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6151 - val_loss: 0.6647 - val_accuracy: 0.5564\n","Epoch 3/20\n","409/409 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6613 - val_loss: 0.6487 - val_accuracy: 0.5928\n","Epoch 4/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.6311 - accuracy: 0.6947 - val_loss: 0.6339 - val_accuracy: 0.6277\n","Epoch 5/20\n","409/409 [==============================] - 2s 4ms/step - loss: 0.6168 - accuracy: 0.7218 - val_loss: 0.6203 - val_accuracy: 0.6537\n","Epoch 6/20\n","409/409 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.7409 - val_loss: 0.6076 - val_accuracy: 0.6791\n","Epoch 7/20\n","409/409 [==============================] - 1s 3ms/step - loss: 0.5912 - accuracy: 0.7536 - val_loss: 0.5959 - val_accuracy: 0.7017\n","Epoch 8/20\n","409/409 [==============================] - 1s 4ms/step - loss: 0.5798 - accuracy: 0.7654 - val_loss: 0.5851 - val_accuracy: 0.7216\n","Epoch 9/20\n","409/409 [==============================] - 1s 3ms/step - loss: 0.5692 - accuracy: 0.7752 - val_loss: 0.5749 - val_accuracy: 0.7360\n","Epoch 10/20\n","409/409 [==============================] - 1s 4ms/step - loss: 0.5593 - accuracy: 0.7811 - val_loss: 0.5655 - val_accuracy: 0.7455\n","Epoch 11/20\n","409/409 [==============================] - 2s 6ms/step - loss: 0.5500 - accuracy: 0.7865 - val_loss: 0.5568 - val_accuracy: 0.7565\n","Epoch 12/20\n","409/409 [==============================] - 3s 8ms/step - loss: 0.5414 - accuracy: 0.7911 - val_loss: 0.5486 - val_accuracy: 0.7626\n","Epoch 13/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.5333 - accuracy: 0.7957 - val_loss: 0.5409 - val_accuracy: 0.7669\n","Epoch 14/20\n","409/409 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7992 - val_loss: 0.5337 - val_accuracy: 0.7745\n","Epoch 15/20\n","409/409 [==============================] - 1s 4ms/step - loss: 0.5186 - accuracy: 0.8019 - val_loss: 0.5270 - val_accuracy: 0.7794\n","Epoch 16/20\n","409/409 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.8048 - val_loss: 0.5206 - val_accuracy: 0.7849\n","Epoch 17/20\n","409/409 [==============================] - 1s 4ms/step - loss: 0.5055 - accuracy: 0.8076 - val_loss: 0.5147 - val_accuracy: 0.7880\n","Epoch 18/20\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4996 - accuracy: 0.8096 - val_loss: 0.5091 - val_accuracy: 0.7911\n","Epoch 19/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.4939 - accuracy: 0.8111 - val_loss: 0.5038 - val_accuracy: 0.7929\n","Epoch 20/20\n","409/409 [==============================] - 2s 6ms/step - loss: 0.4886 - accuracy: 0.8129 - val_loss: 0.4987 - val_accuracy: 0.7923\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7f046c4a8730>"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"WAKPoof9XyDd"},"source":["The simple linear model has only 257 trainable parameters (256 input weights, plus the single bias term). But, it does pretty well at classifying our data, given its simplicity.\n","\n","After 20 epochs of training, my linear model achieved about 80% classification accuracy on the training data, and about 78% classification accuracy on the validation data.\n","\n","That's a pretty reasonable 'baseline', and it suggests that the *majority* of information required for disease-risk prediction is available in a simple linear model.\n","\n","Let's see if we can do better using slightly more complex approaches."]},{"cell_type":"markdown","metadata":{"id":"gqMA5a8V_eMI"},"source":["## convolution model\n","\n","The next code cell is an end-to-end example using a simple nonlinear convolution network for disease-risk prediction.\n","\n","We're using 16 convolution 'filters' of size (3,) in our one-layer network. We've also indicated 'same' padding, so the output of the convolution layer will be the *same* length as the data sequence (256 entries).\n","\n","We decided to use hyperbolic tangent (tanh) nonlinear activation in this convolution layer. Although this activation is a bit unusual for modern-day convolutions (which more commonly use ReLU activation), it matches the default activation used by recurrent neural networks like LSTMs, so we can more directly compare the results from this convolution network with recurrent network models.\n","\n","This model will have 4161 trainable parameters, quite a few more than the simple linear model, so we'll train it for a bit longer (50 epochs, in this case)."]},{"cell_type":"code","metadata":{"id":"93rSQ41XYduw","executionInfo":{"status":"ok","timestamp":1699853223333,"user_tz":300,"elapsed":103725,"user":{"displayName":"","userId":""}},"outputId":"23d1176c-2ced-418a-8831-bb431b0b5316","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas\n","import numpy as np\n","import tensorflow as tf\n","\n","# download data\n","dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n","\n","# create train-validate split\n","train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n","valid_dataframe = dataframe.drop(train_dataframe.index)\n","\n","# extract explanatory variables\n","dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n","train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n","valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n","\n","# extract labels\n","train_y = train_dataframe['LBL0'].to_numpy()\n","valid_y = valid_dataframe['LBL0'].to_numpy()\n","\n","# package data into tensorflow dataset\n","train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n","valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n","\n","# build model\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Conv1D(filters=16,\n","                                 kernel_size=(3,),\n","                                 activation=tf.keras.activations.tanh,\n","                                 padding='same',\n","                                 input_shape=(256,1)))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid))\n","model.summary()\n","\n","# compile model\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","# fit model\n","model.fit(train_data, epochs=50, validation_data=valid_data)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 256, 16)           64        \n","                                                                 \n"," flatten_1 (Flatten)         (None, 4096)              0         \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 4097      \n","                                                                 \n","=================================================================\n","Total params: 4161 (16.25 KB)\n","Trainable params: 4161 (16.25 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/50\n","409/409 [==============================] - 7s 4ms/step - loss: 0.6790 - accuracy: 0.5650 - val_loss: 0.6397 - val_accuracy: 0.7082\n","Epoch 2/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.6111 - accuracy: 0.6804 - val_loss: 0.5969 - val_accuracy: 0.6574\n","Epoch 3/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.5505 - accuracy: 0.7328 - val_loss: 0.5546 - val_accuracy: 0.7146\n","Epoch 4/50\n","409/409 [==============================] - 1s 4ms/step - loss: 0.5118 - accuracy: 0.7616 - val_loss: 0.5259 - val_accuracy: 0.7409\n","Epoch 5/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4865 - accuracy: 0.7789 - val_loss: 0.5052 - val_accuracy: 0.7562\n","Epoch 6/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4668 - accuracy: 0.7914 - val_loss: 0.4887 - val_accuracy: 0.7635\n","Epoch 7/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4514 - accuracy: 0.7998 - val_loss: 0.4758 - val_accuracy: 0.7709\n","Epoch 8/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4393 - accuracy: 0.8054 - val_loss: 0.4658 - val_accuracy: 0.7745\n","Epoch 9/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4296 - accuracy: 0.8112 - val_loss: 0.4579 - val_accuracy: 0.7807\n","Epoch 10/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.4217 - accuracy: 0.8159 - val_loss: 0.4514 - val_accuracy: 0.7840\n","Epoch 11/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.4150 - accuracy: 0.8185 - val_loss: 0.4461 - val_accuracy: 0.7856\n","Epoch 12/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4094 - accuracy: 0.8210 - val_loss: 0.4415 - val_accuracy: 0.7883\n","Epoch 13/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4046 - accuracy: 0.8219 - val_loss: 0.4377 - val_accuracy: 0.7901\n","Epoch 14/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.4005 - accuracy: 0.8238 - val_loss: 0.4343 - val_accuracy: 0.7920\n","Epoch 15/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3968 - accuracy: 0.8243 - val_loss: 0.4313 - val_accuracy: 0.7935\n","Epoch 16/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3936 - accuracy: 0.8252 - val_loss: 0.4288 - val_accuracy: 0.7950\n","Epoch 17/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3907 - accuracy: 0.8260 - val_loss: 0.4264 - val_accuracy: 0.7969\n","Epoch 18/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3882 - accuracy: 0.8271 - val_loss: 0.4244 - val_accuracy: 0.7966\n","Epoch 19/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3858 - accuracy: 0.8278 - val_loss: 0.4225 - val_accuracy: 0.7981\n","Epoch 20/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3837 - accuracy: 0.8289 - val_loss: 0.4208 - val_accuracy: 0.7987\n","Epoch 21/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3818 - accuracy: 0.8300 - val_loss: 0.4192 - val_accuracy: 0.8012\n","Epoch 22/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3800 - accuracy: 0.8308 - val_loss: 0.4177 - val_accuracy: 0.8033\n","Epoch 23/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3784 - accuracy: 0.8320 - val_loss: 0.4163 - val_accuracy: 0.8058\n","Epoch 24/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3769 - accuracy: 0.8327 - val_loss: 0.4150 - val_accuracy: 0.8067\n","Epoch 25/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3755 - accuracy: 0.8329 - val_loss: 0.4138 - val_accuracy: 0.8064\n","Epoch 26/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3742 - accuracy: 0.8337 - val_loss: 0.4126 - val_accuracy: 0.8079\n","Epoch 27/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3730 - accuracy: 0.8340 - val_loss: 0.4115 - val_accuracy: 0.8079\n","Epoch 28/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3719 - accuracy: 0.8346 - val_loss: 0.4105 - val_accuracy: 0.8079\n","Epoch 29/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3709 - accuracy: 0.8349 - val_loss: 0.4094 - val_accuracy: 0.8073\n","Epoch 30/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3699 - accuracy: 0.8350 - val_loss: 0.4084 - val_accuracy: 0.8082\n","Epoch 31/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3690 - accuracy: 0.8354 - val_loss: 0.4075 - val_accuracy: 0.8091\n","Epoch 32/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3681 - accuracy: 0.8354 - val_loss: 0.4066 - val_accuracy: 0.8091\n","Epoch 33/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3673 - accuracy: 0.8357 - val_loss: 0.4056 - val_accuracy: 0.8097\n","Epoch 34/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3666 - accuracy: 0.8358 - val_loss: 0.4048 - val_accuracy: 0.8094\n","Epoch 35/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3659 - accuracy: 0.8360 - val_loss: 0.4039 - val_accuracy: 0.8100\n","Epoch 36/50\n","409/409 [==============================] - 1s 4ms/step - loss: 0.3652 - accuracy: 0.8365 - val_loss: 0.4031 - val_accuracy: 0.8106\n","Epoch 37/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3645 - accuracy: 0.8373 - val_loss: 0.4023 - val_accuracy: 0.8110\n","Epoch 38/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3639 - accuracy: 0.8376 - val_loss: 0.4015 - val_accuracy: 0.8110\n","Epoch 39/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3633 - accuracy: 0.8382 - val_loss: 0.4007 - val_accuracy: 0.8116\n","Epoch 40/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3628 - accuracy: 0.8389 - val_loss: 0.3999 - val_accuracy: 0.8122\n","Epoch 41/50\n","409/409 [==============================] - 1s 4ms/step - loss: 0.3623 - accuracy: 0.8390 - val_loss: 0.3992 - val_accuracy: 0.8125\n","Epoch 42/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3618 - accuracy: 0.8391 - val_loss: 0.3984 - val_accuracy: 0.8134\n","Epoch 43/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3613 - accuracy: 0.8394 - val_loss: 0.3977 - val_accuracy: 0.8146\n","Epoch 44/50\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3609 - accuracy: 0.8395 - val_loss: 0.3970 - val_accuracy: 0.8158\n","Epoch 45/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3604 - accuracy: 0.8397 - val_loss: 0.3963 - val_accuracy: 0.8158\n","Epoch 46/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3600 - accuracy: 0.8396 - val_loss: 0.3956 - val_accuracy: 0.8158\n","Epoch 47/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3596 - accuracy: 0.8395 - val_loss: 0.3950 - val_accuracy: 0.8158\n","Epoch 48/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3593 - accuracy: 0.8396 - val_loss: 0.3943 - val_accuracy: 0.8158\n","Epoch 49/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3589 - accuracy: 0.8400 - val_loss: 0.3937 - val_accuracy: 0.8168\n","Epoch 50/50\n","409/409 [==============================] - 2s 4ms/step - loss: 0.3586 - accuracy: 0.8400 - val_loss: 0.3930 - val_accuracy: 0.8168\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7f046d860b20>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"P7xlS_XXYeLG"},"source":["Keep track of the time needed to train each epoch, as well as the final accuracy and val_accuracy values, for the associated quiz."]},{"cell_type":"markdown","metadata":{"id":"z4ZyqiR5BWOx"},"source":["## LSTM model\n","\n","Finally, we'll try using an LSTM model to analyze the sequence data.\n","\n","Implement a single-layer LSTM model in the following code cell, using 16 LSTM 'units', and make sure you set return_sequences=True in your model. The rest of the model should use the same Dense decision layer as before.\n","\n","Remember that LSTM networks can take much longer to train!"]},{"cell_type":"code","metadata":{"id":"ijrxhUsmZXtB","outputId":"8fb17853-336f-4e27-a793-8ead38c29974","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas\n","import numpy as np\n","import tensorflow as tf\n","\n","# download data\n","dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n","\n","# create train-validate split\n","train_dataframe = dataframe.sample(frac=0.8, random_state=2100963)\n","valid_dataframe = dataframe.drop(train_dataframe.index)\n","\n","# extract explanatory variables\n","dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n","train_x = np.expand_dims(train_dataframe[dta_ids].to_numpy(), axis=-1)\n","valid_x = np.expand_dims(valid_dataframe[dta_ids].to_numpy(), axis=-1)\n","\n","# extract labels\n","train_y = train_dataframe['LBL0'].to_numpy()\n","valid_y = valid_dataframe['LBL0'].to_numpy()\n","\n","# package data into tensorflow dataset\n","train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n","valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n","\n","# build model\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.LSTM(units=16, return_sequences=True, input_shape=(256,1)))\n","model.add(tf.keras.layers.Flatten())\n","model.summary()\n","\n","# compile model\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","# fit model\n","model.fit(train_data, epochs=50, validation_data=valid_data)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 256, 16)           1152      \n","                                                                 \n"," flatten_2 (Flatten)         (None, 4096)              0         \n","                                                                 \n","=================================================================\n","Total params: 1152 (4.50 KB)\n","Trainable params: 1152 (4.50 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/50\n","409/409 [==============================] - 11s 14ms/step - loss: 1.5360 - accuracy: 0.0000e+00 - val_loss: 0.7670 - val_accuracy: 0.0000e+00\n","Epoch 2/50\n","409/409 [==============================] - 4s 11ms/step - loss: 0.7459 - accuracy: 0.0000e+00 - val_loss: 0.7388 - val_accuracy: 0.0000e+00\n","Epoch 3/50\n","409/409 [==============================] - 4s 11ms/step - loss: 0.7291 - accuracy: 0.0000e+00 - val_loss: 0.7264 - val_accuracy: 0.0000e+00\n","Epoch 4/50\n","409/409 [==============================] - 5s 13ms/step - loss: 0.7206 - accuracy: 0.0000e+00 - val_loss: 0.7168 - val_accuracy: 0.0000e+00\n","Epoch 5/50\n","377/409 [==========================>...] - ETA: 0s - loss: 0.7131 - accuracy: 0.0000e+00"]}]},{"cell_type":"markdown","metadata":{"id":"qbKTMcfrZYHh"},"source":["Make sure you keep track of the training time per epoch, and the final accuracy and loss values, for the associated quiz."]},{"cell_type":"markdown","metadata":{"id":"8kj0H4SDANM9"},"source":["Now that you've tried some 'baseline' models, including a simple linear model and some one-layer convolution and LSTM models, you might want to try 'playing around' with your models, to see if you can improve accuracy without causing too much overfitting.\n","\n","What happens to the model's accuracy when you increase the number of filters (for convolution networks) or units (for LSTM networks)?\n","\n","What happens if you add more layers to the networks?\n","\n","Does training for a longer number of epochs improve accuracy?\n","\n","When you increase the size of your networks, at what point do you start to see overfitting? Can you 'fix' overfitting with Dropout layers or regularization?\n","\n","How accurate can you make your model, without too much overfitting?"]}]}
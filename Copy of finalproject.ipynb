{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/raqueldias/ALS3200C/blob/main/finalproject.ipynb","timestamp":1702009711248}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lOY8dbu52Z2H"},"source":["# microbiome disease prediction with transformers\n","\n","The final course project is designed to be an open-ended and 'creative' analysis of micobial community data, with the goal of training a model to predict autoimmune disease risk from microbiome data.\n","\n","The data intake and packaging pipeline is coded for you in the following code cell; it includes the addition of 'location' information in the sequence data, so it can be directly used in transformer models.\n","\n","We will be using the same microbial community data that was used in week 12, and you will be required to implement a transformer model similar to the one we implemented in week 13, so you may want to review the material from those weeks before getting started.\n","\n","But this final project is a chance for you to 'play around' with *all* the various tools and techniques we've looked at throughout this course, so take a little time to reflect on some of the problems and approaches we've worked on; these are all 'tools' you can use for data analysis.\n","\n","The grade for the final project will be based on a structured 'write up' of what you do in this jupyter notebook. This is a chance for you to explore the concepts and techniques we've introduced in this course in more detail, and get points for doing so. So think creatively!\n","\n","I'd recommend exploring ideas and analysis options by conducting a series of 'experiments' or data analyses. In each 'experiment', you make a *change* to *how* you are analyzing the data. You might use a different optimizer, or add another layer to your network. *Which* changes you make should be guided by *questions* or *problems* you encounter. For example, your model's accuracy might be low, and you might try to *improve* accuracy by adding a new layer to your network. For another example, if you observe overfitting in your model, you might try adding a regularizer to reduce overfitting. You then observe the *effect* of each change you make on the data anaysis, so you can evaluate what the change does. For example, does adding another layer to the network improve accuracy? Does it also increase overfitting?\n","\n","For each 'experiment' you conduct in this notebook, you'll want to keep track of the following information:\n","\n","1. What *question* is the experiment or analysis trying to answer? What do you want to *know* by doing the analysis or experiment?\n","2. What exactly did you *do*? What changes did you make that you want to assess?\n","3. What happened? What information or 'data' was collected during your analysis or experiment?\n","4. What did the 'results' of your experiment or analysis *tell you* that is *relevant* to answering the question that prompted you to perform this experiment or analysis?\n","\n","Every time you do an analysis or change something about the way you are modeling or analyzing the data, keep track of your answers to questions 1-4 above, and then you can use that information for your final project write up."]},{"cell_type":"code","metadata":{"id":"hhTYvt3Z2Y5K","executionInfo":{"status":"ok","timestamp":1702251696056,"user_tz":300,"elapsed":11515,"user":{"displayName":"Yash Hegde","userId":"17937778572476104408"}},"outputId":"18213470-6831-4fa0-9042-4f2efd1561ce","colab":{"base_uri":"https://localhost:8080/"}},"source":["import pandas\n","import numpy as np\n","import tensorflow as tf\n","\n","# download data\n","dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n","\n","# create train-validate split\n","train_dataframe = dataframe.sample(frac=0.8, random_state=827847)\n","valid_dataframe = dataframe.drop(train_dataframe.index)\n","\n","# extract explanatory variables\n","dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n","train_x = train_dataframe[dta_ids].to_numpy()\n","valid_x = valid_dataframe[dta_ids].to_numpy()\n","\n","# add 'location' to sequence data\n","loc = np.linspace(start=-2.5, stop=+2.5, num=train_x.shape[1])\n","train_x = np.stack([ train_x, np.array([loc]*train_x.shape[0]) ], axis=-1)\n","valid_x = np.stack([ valid_x, np.array([loc]*valid_x.shape[0]) ], axis=-1)\n","\n","# extract labels\n","train_y = train_dataframe['LBL0'].to_numpy()\n","valid_y = valid_dataframe['LBL0'].to_numpy()\n","\n","# package data into tensorflow dataset\n","train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n","valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n","\n","print(train_data, valid_data)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["<_BatchDataset element_spec=(TensorSpec(shape=(None, 256, 2), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))> <_BatchDataset element_spec=(TensorSpec(shape=(None, 256, 2), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>\n"]}]},{"cell_type":"markdown","metadata":{"id":"uAxsaZVb3VgO"},"source":["From the Dataset information printed to the screen, you can see that the shape of your input data is (256,2). You might also want to note that you have a binary classification problem.\n","\n","The data intake and packaging pipeline is reproduced in the following code cell. Your job is to implement the code that builds, compiles and fits a transformer model to these data, including model evaluation.\n","\n","I'd recommend starting with a very 'simple' transformer model, to get a baseline accuracy measurement.\n","\n","You might also want to try changing the random seed used to create the train-validate data split, so you can evaluate your model's accuracy using *different* train-validate splits. For example, if you *average* your accuracy measurements over *many different* train-validate splits, you'll probably get a better estimate of your model's *expected* accuracy (this is called \"cross-validation\", and it is commonly used in machine learning).\n","\n","You might also want to try changing the number of epochs used for training, to see how short you can make your training runs, while still training your model 'to convergence' (ie, stable loss values). You could also try using different optimizers, to see if that impacts how long you need to train your model.\n","\n","After you get a good 'feeling' for the behaviour of your 'baseline' transformer model for this problem, try altering the model in various ways, and observe the impact on training time and accuracy. For example, you will probably want to try changing the dimensionality of the internal data representation in your model. You will probably also want to try changing the number of attention 'heads' in your multi-head attention layer. You could also try removing the normalization layers in your model, to see if layer normalization is needed.\n","\n","You may also want to try replicating the *entire* transformer block multiple times; try creating a model with 2 or 4 sequential transformer blocks, one after the other, before the decision layers. At what point does your model become 'too complex' and start 'overfitting' the training data? Does adding dropout or regularization help control overfitting in your transformer model?\n","\n","Make sure you keep track of each change you try, and the effect of the various changes. Remember that you can add new text and code cells to this jupyter notebook, so you could replicate the code cell below to try out different things, and add text cells to keep notes of what you are trying out, why and the results of each 'experiment'.\n","\n","Finally, you might want to consider comparing the transformer model to alternative sequence models for this problem. How does the transformer model's accuracy compare to the accuracy of a Conv1D or LSTM model? Which models are faster to train? Which models have more trainable parameters? Are any models inherently more/less susceptible to overfitting? You might want to also try testing variants of Conv1D and LSTM that *include* the linear projection layer that projects your data into a high-dimensional representational space, so the results are more 'comparable' between the transformer and the other sequence models."]},{"cell_type":"code","metadata":{"id":"-lfUJnUu3WHz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"107c4256-c825-4bb5-94be-937ee196e0a2","executionInfo":{"status":"ok","timestamp":1702257249957,"user_tz":300,"elapsed":70455,"user":{"displayName":"Yash Hegde","userId":"17937778572476104408"}}},"source":["import pandas\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","# download data\n","dataframe = pandas.read_csv('https://raw.githubusercontent.com/bryankolaczkowski/ALS3200C/main/mbiome.data.csv')\n","\n","\n","# create train-validate split\n","train_dataframe = dataframe.sample(frac=0.8, random_state=1000000)\n","valid_dataframe = dataframe.drop(train_dataframe.index)\n","\n","\n","# extract explanatory variables\n","dta_ids = [ x for x in dataframe.columns if x.find('DTA') == 0 ]\n","train_x = train_dataframe[dta_ids].to_numpy()\n","valid_x = valid_dataframe[dta_ids].to_numpy()\n","\n","\n","# add 'location' to sequence data\n","loc = np.linspace(start=-2.5, stop=+2.5, num=train_x.shape[1])\n","train_x = np.stack([ train_x, np.array([loc]*train_x.shape[0]) ], axis=-1)\n","valid_x = np.stack([ valid_x, np.array([loc]*valid_x.shape[0]) ], axis=-1)\n","\n","\n","# extract labels\n","train_y = train_dataframe['LBL0'].to_numpy()\n","valid_y = valid_dataframe['LBL0'].to_numpy()\n","\n","\n","# package data into tensorflow dataset\n","train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32)\n","valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y)).batch(32)\n","\n","\n","## build model using functional api\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Conv1D(filters=16,\n","                                 kernel_size=(3,),\n","                                 activation=tf.keras.activations.tanh,\n","                                 padding='same',\n","                                 input_shape=(256,2)))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(units=1, activation=tf.keras.activations.sigmoid))\n","model.summary()\n","\n","\n","# classification block\n","\n","\n","\n","\n","## compile model\n","model.compile(optimizer=tf.keras.optimizers.Adam(),\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=['accuracy'])\n","\n","\n","## fit model\n","model.fit(train_data, epochs=20, validation_data=valid_data)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 256, 16)           112       \n","                                                                 \n"," flatten_1 (Flatten)         (None, 4096)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 4097      \n","                                                                 \n","=================================================================\n","Total params: 4209 (16.44 KB)\n","Trainable params: 4209 (16.44 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20\n","409/409 [==============================] - 5s 8ms/step - loss: 0.6461 - accuracy: 0.6306 - val_loss: 0.5907 - val_accuracy: 0.6669\n","Epoch 2/20\n","409/409 [==============================] - 3s 7ms/step - loss: 0.4984 - accuracy: 0.7938 - val_loss: 0.4675 - val_accuracy: 0.7880\n","Epoch 3/20\n","409/409 [==============================] - 2s 6ms/step - loss: 0.4167 - accuracy: 0.8246 - val_loss: 0.4225 - val_accuracy: 0.8054\n","Epoch 4/20\n","409/409 [==============================] - 2s 6ms/step - loss: 0.3827 - accuracy: 0.8351 - val_loss: 0.4114 - val_accuracy: 0.8076\n","Epoch 5/20\n","409/409 [==============================] - 2s 6ms/step - loss: 0.3670 - accuracy: 0.8426 - val_loss: 0.4051 - val_accuracy: 0.8082\n","Epoch 6/20\n","409/409 [==============================] - 3s 6ms/step - loss: 0.3579 - accuracy: 0.8462 - val_loss: 0.3993 - val_accuracy: 0.8137\n","Epoch 7/20\n","409/409 [==============================] - 4s 9ms/step - loss: 0.3519 - accuracy: 0.8475 - val_loss: 0.3936 - val_accuracy: 0.8207\n","Epoch 8/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3483 - accuracy: 0.8476 - val_loss: 0.3894 - val_accuracy: 0.8232\n","Epoch 9/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3462 - accuracy: 0.8478 - val_loss: 0.3867 - val_accuracy: 0.8238\n","Epoch 10/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3447 - accuracy: 0.8493 - val_loss: 0.3850 - val_accuracy: 0.8250\n","Epoch 11/20\n","409/409 [==============================] - 2s 6ms/step - loss: 0.3438 - accuracy: 0.8499 - val_loss: 0.3837 - val_accuracy: 0.8284\n","Epoch 12/20\n","409/409 [==============================] - 4s 9ms/step - loss: 0.3431 - accuracy: 0.8502 - val_loss: 0.3827 - val_accuracy: 0.8266\n","Epoch 13/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3425 - accuracy: 0.8501 - val_loss: 0.3819 - val_accuracy: 0.8293\n","Epoch 14/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3421 - accuracy: 0.8498 - val_loss: 0.3812 - val_accuracy: 0.8302\n","Epoch 15/20\n","409/409 [==============================] - 2s 6ms/step - loss: 0.3418 - accuracy: 0.8501 - val_loss: 0.3806 - val_accuracy: 0.8308\n","Epoch 16/20\n","409/409 [==============================] - 3s 6ms/step - loss: 0.3415 - accuracy: 0.8499 - val_loss: 0.3800 - val_accuracy: 0.8302\n","Epoch 17/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3412 - accuracy: 0.8500 - val_loss: 0.3795 - val_accuracy: 0.8299\n","Epoch 18/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3410 - accuracy: 0.8502 - val_loss: 0.3789 - val_accuracy: 0.8305\n","Epoch 19/20\n","409/409 [==============================] - 2s 5ms/step - loss: 0.3409 - accuracy: 0.8509 - val_loss: 0.3784 - val_accuracy: 0.8296\n","Epoch 20/20\n","409/409 [==============================] - 3s 7ms/step - loss: 0.3407 - accuracy: 0.8506 - val_loss: 0.3779 - val_accuracy: 0.8290\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7a15e5e8e230>"]},"metadata":{},"execution_count":5}]}]}